import numpy as np
import pandas as pd
import matplotlib .pyplot as plt
import sklearn
import seaborn as sns
import time

# Ignore warnings
import warnings
warnings.filterwarnings('ignore')
# Import training and validation data
df = pd.read_csv("../input/bluebook-for-bulldozers/TrainAndValid.csv")
df_test = pd.read_csv("../input/bluebook-for-bulldozers/Valid.csv")
Evaluation
Things to evalute for this data

df.head()
df.info()
df.describe()
df.isnull().sum()
fig, ax = plt.subplots(figsize=(10,6))
ax.scatter(df['saledate'][:1000], df['SalePrice'][:1000]);
df.SalePrice.plot.hist(figsize=(10,6));
When we work with time series data, we want to enrich the time & date component as much as possible.
We can do that by telling pandas which of our columns has dates in it using parse_dates parameter.

# Import data again but this time parse dates
df = pd.read_csv("../input/bluebook-for-bulldozers/TrainAndValid.csv",
                  parse_dates=['saledate'])
Evaluate the updated import
df.saledate.dtype
df.saledate[:1000]
df.saledate.head(20)
fig, ax = plt.subplots(figsize=(12,7))
ax.scatter(df['saledate'][:1000], df["SalePrice"][:1000]);
Sort DataFrame by saledate
When working with time series data, it's a good idea to sort by date.

# Sort DataFrame in date order
df.sort_values(by=["saledate"], inplace=True, ascending=True) # inplace=True is used so we don't need to do df = df.sort_values
# This will sort our DataFrame by saledate, starting with the earliest date first
df.saledate.head(20)
Make a copy of the original DataFrame
Make a copy of the original DataFrame so when we manipulate the copied version we've still got the original data

# Make a copy of the original DataFrame
df_tmp = df.copy()
Add datetime parameters for saledate column
df_tmp["saleYear"] = df_tmp.saledate.dt.year
df_tmp["saleMonth"] = df_tmp.saledate.dt.month
df_tmp["saleDay"] = df_tmp.saledate.dt.day
df_tmp["saleDayOfWeek"] = df_tmp.saledate.dt.dayofweek
df_tmp["saleDayOfYear"] = df_tmp.saledate.dt.dayofyear
# Now we've enriched our DataFrame with date time features, we can remove 'saledate'
df_tmp.drop('saledate', axis=1, inplace=True)
Modeling
Starting model-driven EDA

df_tmp.head()
# Let's build a machine learning model
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_jobs=-1,
                              random_state=42)

model.fit(df_tmp.drop('SalePrice', axis=1), df_tmp["SalePrice"])
pd.api.types.is_string_dtype(df_tmp["UsageBand"])
# Build a for loop that finds columns that contain strings
for label, content in df_tmp.items():
    if pd.api.types.is_string_dtype(content):
        print(label)

# This shows all of the columns that are strings
# This will turn all of the string values into category values
for label, content in df_tmp.items():
    if pd.api.types.is_string_dtype(content):
        df_tmp[label] = content.astype('category').cat.as_ordered()
        
# as_ordered puts things in alphabetical order
# We have changed all of the columns that were strings into categories
df_tmp.info()
Thanks to pandas categories we now have a way to access all of our data in the form of numbers

But we still have a bunch of missing data...

# Check missing data
df_tmp.isnull().sum()/len(df_tmp) # divide by the length of the DataFrame so we see a % of missing data
Save preprocessed data
# Export current tmp DataFrame
df_tmp.to_csv("train_tmp.csv",
              index=False)
# Import preprocessed data
df_tmp.to_csv = pd.read_csv("train_tmp.csv",
                             low_memory=False)
df_tmp.isna().sum()
Fill Missing Values
Fill numerical missing values first
for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content): # IF the content in the columns is numberic ... print out the label
        print(label)
# Check for which numeric columns have null values
for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            print(label)
# Fill the numberic rows with the median
for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            # Add a binary column which tells us if the data was missing or not
            df_tmp[label+"_is_missing"] = pd.isnull(content)
            # Fill missing numberic values with median
            df_tmp[label] = content.fillna(content.median())
# Check if there's any null numberic values
for label, content in df_tmp.items():
    if pd.api.types.is_numeric_dtype(content):
        if pd.isnull(content).sum():
            print(label)
# Check to see how many examples were missing
df_tmp.auctioneerID_is_missing.value_counts()
Filling and turning categorical variables into numbers
# Check for columns which aren't numberic
for label, content in df_tmp.items():
    if not pd.api.types.is_numeric_dtype(content):
        print(label)
# Turn categorical variables into numbers and fill missing
for label, content in df_tmp.items():
    if not pd.api.types.is_numeric_dtype(content):
        # Add binary column to indicate whether sample had missing value
        df_tmp[label+"_is_missing"] = pd.isnull(content)
        # Turn categories into numbers and add +1
        df_tmp[label] = pd.Categorical(content).codes +1
pd.Categorical(df_tmp['state']).codes+1
df_tmp.info()
# We can see all of our categorical missing columns now
df_tmp.head().T
df_tmp.isnull().sum()
Now that all of our data is numberic as well as our DataFrame has no missing values, we should be able to build a machine learning model.

df_tmp.head()
%%time
# Instantiate model on all 412,000 rows
model = RandomForestRegressor(n_jobs=-1,
                              random_state=42)

# Fit the model
model.fit(df_tmp.drop('SalePrice', axis=1), df_tmp['SalePrice'])
# Score the model
model.score(df_tmp.drop('SalePrice', axis=1), df_tmp['SalePrice'])
Splitting data into train / validation sets
df_tmp.saleYear
df_tmp.saleYear.value_counts()
# Split data into training and validation
df_val = df_tmp[df_tmp.saleYear == 2012]
df_train = df_tmp[df_tmp.saleYear != 2012]

len(df_val), len(df_train)
# The validation set contains all data where the sale year is in 2012 (as per Kaggle competition rules)
# The train set is everything that is not 2012 for the sale year
# Split the data into X & y
X_train, y_train = df_train.drop('SalePrice', axis =1 ), df_train.SalePrice
X_valid, y_valid = df_val.drop('SalePrice', axis=1), df_val.SalePrice

X_train.shape, y_train.shape, X_valid.shape, y_valid.shape
Building an evaluation function
# Create a valuiation function (the competition uses RMSLE)
from sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score

def rmsle(y_test, y_preds):
    """
    Calculates root mean squared log error between predictions and true labels.
    """
    return np.sqrt(mean_squared_log_error(y_test, y_preds))

# Create function to evaluate model on a few different levels
def show_scores(model):
    train_preds = model.predict(X_train)
    val_preds = model.predict(X_valid)
    scores = {"Training MAE": mean_absolute_error(y_train, train_preds),
              'Valid MAE': mean_absolute_error(y_valid, val_preds),
              'Training RMSLE': rmsle(y_train, train_preds),
              "Valid RMSLE": rmsle(y_valid, val_preds),
              "Training R^2": r2_score(y_train, train_preds),
              'Valid R^2': r2_score(y_valid, val_preds)}
    return scores
Testing our model on a subset (to tune the hyperparameters)
# Change max_samples value
model = RandomForestRegressor(n_jobs=-1,
                              random_state=42,
                              max_samples=10000)
%%time
# Cutting down on the max number of samples each estimator can see improves training time
model.fit(X_train, y_train)
show_scores(model)
Hyperparameter tuning with RandomizedSearchCV
%%time
from sklearn.model_selection import RandomizedSearchCV

# Different RandomForestRegressor Hyperparameters
rf_grid = {"n_estimators": np.arange(10, 100, 10),
           "max_depth": [None, 3, 5, 10],
           "min_samples_split": np.arange(2, 20, 2),
           "min_samples_leaf": np.arange(1, 20, 2),
           "max_features": [0.5, 1, 'sqrt', 'auto'],
           "max_samples": [10000]}

# Instantiate RandomizedSearchCV Model
rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,
                                                    random_state=42),
                              param_distributions=rf_grid,
                              n_iter=10,
                              cv=5,
                              verbose=True)

# Fit the RandomizedSearchCV Model
rs_model.fit(X_train, y_train)
# Find the best model hyperparameters 
rs_model.best_params_
show_scores(rs_model)
show_scores(model)
%%time

# Most ideal hyperparameters
ideal_model = RandomForestRegressor(n_estimators=40,
                                    min_samples_leaf=1,
                                    min_samples_split=14,
                                    max_features=0.5,
                                    n_jobs=-1,
                                    max_samples=None,
                                   random_state=42)

# Fit the RandomizedSearchCV Model
ideal_model.fit(X_train, y_train)
# Scores for ideal_model trained on all of the data
show_scores(ideal_model)
# Scores on rs_model only trainedon 10,000 examples
show_scores(rs_model)
df_test = pd.read_csv('../input/bluebook-for-bulldozers/Test.csv',
                      low_memory=False,
                      parse_dates=["saledate"])

df_test.head()
Preprocessing the data (getting the test dataset in the same format as our training dataset)
def preprocess_data(df):
    """
    Performs transformations on df and returns transformed df.
    """
    df["saleYear"] = df.saledate.dt.year
    df["saleMonth"] = df.saledate.dt.month
    df["saleDay"] = df.saledate.dt.day
    df["saleDayOfWeek"] = df.saledate.dt.dayofweek
    df["saleDayOfYear"] = df.saledate.dt.dayofyear
    
    df.drop("saledate", axis=1, inplace=True)
    
    # Fill the numeric rows with median
    for label, content in df.items():
        if pd.api.types.is_numeric_dtype(content):
            if pd.isnull(content).sum():
                # Add a binary column which tells us if the data was missing or not
                df[label+"_is_missing"] = pd.isnull(content)
                # Fill missing numeric values with median
                df[label] = content.fillna(content.median())
    
        # Filled categorical missing data and turn categories into numbers
        if not pd.api.types.is_numeric_dtype(content):
            df[label+"_is_missing"] = pd.isnull(content)
            # We add +1 to the category code because pandas encodes missing categories as -1
            df[label] = pd.Categorical(content).codes+1
    
    return df
# Process the test data 
df_test = preprocess_data(df_test)
df_test.head()
# We can find how the columns differ using sets
set(X_train.columns) - set(df_test.columns)
df_test['auctioneerID_is_missing'] = False
df_test.head()
# Make predictions on the test data
test_preds = ideal_model.predict(df_test)
test_preds
# Format predictions into the same format Kaggle is after
df_preds = pd.DataFrame()
df_preds['SalesID'] = df_test['SalesID']
df_preds['SalesPrice'] = test_preds
df_preds
df_preds.to_csv('./predictions.csv', index=False)
# Find feature importance of best model
ideal_model.feature_importances_
# Helper function for plotting feature importance
def plot_features(columns, importances, n=20):
    df = (pd.DataFrame({"features": columns,
                        "feature_importances": importances})
          .sort_values("feature_importances", ascending=False)
          .reset_index(drop=True))
    
    # Plot the dataframe
    fig, ax = plt.subplots()
    ax.barh(df["features"][:n], df["feature_importances"][:20])
    ax.set_ylabel("Features")
    ax.set_xlabel("Feature importance")
    ax.invert_yaxis()
plot_features(X_train.columns, ideal_model.feature_importances_)
